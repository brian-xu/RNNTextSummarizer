{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9a3854",
   "metadata": {},
   "source": [
    "#### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77483029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pathlib\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.math import top_k\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a52a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "bs4                 4.10.0\n",
       "keras_preprocessing 1.1.2\n",
       "matplotlib          3.5.0\n",
       "nltk                3.6.5\n",
       "numpy               1.19.5\n",
       "pandas              1.4.1\n",
       "session_info        1.0.0\n",
       "tensorflow          2.5.0\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "2f7ece400a652629565c523b34ee61b04afa385c    NA\n",
       "PIL                                         8.4.0\n",
       "absl                                        NA\n",
       "astunparse                                  1.6.3\n",
       "backcall                                    0.2.0\n",
       "bottleneck                                  1.3.2\n",
       "certifi                                     2021.10.08\n",
       "charset_normalizer                          2.0.12\n",
       "colorama                                    0.4.4\n",
       "cycler                                      0.10.0\n",
       "cython_runtime                              NA\n",
       "dateutil                                    2.8.1\n",
       "decorator                                   4.4.2\n",
       "flatbuffers                                 NA\n",
       "gast                                        NA\n",
       "google                                      NA\n",
       "h5py                                        3.1.0\n",
       "idna                                        3.3\n",
       "ipykernel                                   5.3.4\n",
       "ipython_genutils                            0.2.0\n",
       "jedi                                        0.18.1\n",
       "joblib                                      1.1.0\n",
       "kiwisolver                                  1.3.2\n",
       "mpl_toolkits                                NA\n",
       "nt                                          NA\n",
       "ntsecuritycon                               NA\n",
       "numexpr                                     2.8.1\n",
       "opt_einsum                                  v3.3.0\n",
       "packaging                                   21.3\n",
       "parso                                       0.8.0\n",
       "pickleshare                                 0.7.5\n",
       "prompt_toolkit                              3.0.8\n",
       "pygments                                    2.7.1\n",
       "pyparsing                                   3.0.4\n",
       "pytz                                        2021.3\n",
       "regex                                       2.5.97\n",
       "requests                                    2.27.1\n",
       "scipy                                       1.6.2\n",
       "six                                         1.15.0\n",
       "sklearn                                     1.0.2\n",
       "soupsieve                                   2.3.1\n",
       "storemagic                                  NA\n",
       "tensorboard                                 2.8.0\n",
       "termcolor                                   1.1.0\n",
       "threadpoolctl                               3.1.0\n",
       "tornado                                     6.0.4\n",
       "traitlets                                   5.0.5\n",
       "typing_extensions                           NA\n",
       "urllib3                                     1.26.8\n",
       "wcwidth                                     0.2.5\n",
       "win32api                                    NA\n",
       "win32security                               NA\n",
       "wrapt                                       1.12.1\n",
       "zmq                                         19.0.2\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             7.18.1\n",
       "jupyter_client      6.1.7\n",
       "jupyter_core        4.6.3\n",
       "-----\n",
       "Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.19043-SP0\n",
       "-----\n",
       "Session information updated at 2022-03-14 21:18\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb46b1",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "We used the preprocessing code from https://iq.opengenus.org/text-summarization-using-rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b05e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString).text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if (num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1: # removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f976ac",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM Encoder Decoder Model\n",
    "### Load in our model\n",
    "\n",
    "The model we will be using is the encoder decoder model we wrote with 2 bidirectional LSTM layers in the encoder, and 1 LSTM layer in the decoder with an attention layer and dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e0d3db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/enc_embedding.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-96ba4d5f1247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# load the weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mw_encoder_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/enc_embedding.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mw_decoder_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/dec_embedding.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mw_encoder_lstm0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model/encoder_lstm0.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\175\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/enc_embedding.npz'"
     ]
    }
   ],
   "source": [
    "#vocabulary size from when we were training the encoder decoder model\n",
    "x_voc = 8440\n",
    "y_voc = 1989\n",
    "\n",
    "max_text_len=400\n",
    "max_summary_len=46\n",
    "\n",
    "# load the weights\n",
    "w_encoder_embeddings = np.load('model/enc_embedding.npz', allow_pickle=True)\n",
    "w_decoder_embeddings = np.load('model/dec_embedding.npz', allow_pickle=True)\n",
    "w_encoder_lstm0 = np.load('model/encoder_lstm0.npz', allow_pickle=True)\n",
    "w_encoder_lstm1 = np.load('model/encoder_lstm1.npz', allow_pickle=True)\n",
    "w_decoder_lstm = np.load('model/lstm_2.npz', allow_pickle=True)\n",
    "time_distributed  = np.load('model/time_distributed.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91375da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM encoder decoder\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# embedding layer\n",
    "enc_emb =  Embedding(63320, 100, trainable=True, name = \"enc_embedding\")(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder_lstm0 = Bidirectional(\n",
    "    LSTM(latent_dim, return_sequences=True),\n",
    "    name = 'encoder_lstm0')\n",
    "encoder_outputs0 = encoder_lstm0(enc_emb)\n",
    "\n",
    "encoder_lstm1 = Bidirectional(\n",
    "    LSTM(latent_dim, return_state=True),\n",
    "    name = 'encoder_lstm1')\n",
    "encoder_outputs, enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c  = encoder_lstm1(encoder_outputs0)\n",
    "\n",
    "\n",
    "state_h = Concatenate()([enc_forward_h, enc_backward_h])\n",
    "state_c = Concatenate()([enc_forward_c, enc_backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(14856, 100,trainable=True, name = 'dec_embedding')\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "decoder_outputs1, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=encoder_states)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention(name='attention_layer')\n",
    "attn_out = attn_layer([decoder_outputs1, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs1, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = TimeDistributed(Dense(14856, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() \n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights of the model\n",
    "\n",
    "model.layers[1].set_weights(w_encoder_embeddings['arr_0'])\n",
    "model.layers[2].set_weights(w_encoder_lstm0['arr_0'])\n",
    "model.layers[4].set_weights(w_encoder_lstm1['arr_0'])\n",
    "model.layers[5].set_weights(w_decoder_embeddings['arr_0'])\n",
    "model.layers[8].set_weights(w_decoder_lstm['arr_0'])\n",
    "model.layers[11].set_weights(time_distributed['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the tokenizer\n",
    "with open('model/x_tokenizer.pickle', 'rb') as handle:\n",
    "    x_tokenizer = pickle.load(handle)\n",
    "with open('model/y_tokenizer.pickle', 'rb') as handle:\n",
    "    y_tokenizer = pickle.load(handle)\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference model\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "decoder_hidden_state_input = Input(shape=(latent_dim*2))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59534ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def top_k_sampling(predictions, k):\n",
    "    top_k_probabilities, top_k_indices= top_k(predictions, k=k, sorted=True)\n",
    "    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
    "    top_k_redistributed_probability=softmax(np.log(top_k_probabilities))\n",
    "    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
    "    token = np.random.choice(top_k_indices, p=top_k_redistributed_probability)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1905f18",
   "metadata": {},
   "source": [
    "The following code is also code from  https://iq.opengenus.org/text-summarization-using-rnn/ but we changed the sampling methods and added our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b46a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        # Uncomment one of the following lines to change the sampling method\n",
    "        #sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_index = top_k_sampling(output_tokens[0, -1, :],5)\n",
    "        #sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=output_tokens[0, -1, :])\n",
    "  \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee5f6c",
   "metadata": {},
   "source": [
    "### Run some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    x_val_seq   =   x_tokenizer.texts_to_sequences([text])\n",
    "    x_val_i   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "    return decode_sequence(x_val_i[0].reshape(1,max_text_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649dbba9",
   "metadata": {},
   "source": [
    "#### Article\n",
    "Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.Â 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
    "#### Original Summary\n",
    "Experts question if  packed out planes are putting passengers at risk .\n",
    "U.S consumer advisory group says minimum space must be stipulated .\n",
    "Safety tests conducted on planes with more leg room than airlines offer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dfd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it\\'s putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn\\'t stipulate a minimum amount of space for humans. \\'In a world where animals have more rights to space and food than humans,\\' said Charlie Leocha, consumer representative on the committee.Â \\'It is time that the DOT and FAA take a stand for humane treatment of passengers.\\' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson\\'s short haul seat pitch is 28 inches, and Virgin Atlantic\\'s is 30-31.'\n",
    "cleaned_article = text_cleaner(article,0)\n",
    "print('Predicted summary:',summarize(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3651ec6",
   "metadata": {},
   "source": [
    "#### Article\n",
    "A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at theÂ Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'. Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!' A zoo spokesman said: 'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. 'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. 'We then handed him over to the police.' Brave fool: Fortunately, Mr Kumar  fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it. 'I was drunk and thought I'd stand a good chance.' A police spokesman said: 'He has been cautioned and will be sent for psychiatric evaluation. 'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.\n",
    "\n",
    "#### Original Summary\n",
    "Drunk teenage boy climbed into lion enclosure at zoo in west India .\n",
    "Rahul Kumar, 17, ran towards animals shouting 'Today I kill a lion!'\n",
    "Fortunately he fell into a moat before reaching lions and was rescued ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba094118",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'A drunk teenage boy had to be rescued by security after jumping into a lions\\' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at theÂ Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would \\'kill them\\'. Mr Kumar explained afterwards that he was drunk and \\'thought I\\'d stand a good chance\\' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions\\' enclosure at a zoo in Ahmedabad and began running towards the animals shouting \\'Today I kill a lion!\\' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: \\'Today I kill a lion or a lion kills me!\\' A zoo spokesman said: \\'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. \\'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. \\'We then handed him over to the police.\\' Brave fool: Fortunately, Mr Kumar fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: \\'I don\\'t really know why I did it. \\'I was drunk and thought I\\'d stand a good chance.\\' A police spokesman said: \\'He has been cautioned and will be sent for psychiatric evaluation. \\'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.\\' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.'\n",
    "cleaned_article = text_cleaner(article,0)\n",
    "print('Predicted summary:',summarize(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d81e78",
   "metadata": {},
   "source": [
    "## 3 LSTM Encoder Decoder Model\n",
    "### Load in our model\n",
    "\n",
    "We used 3 LSTM layers in the encoder for this mode, with dropout = 0.2. In this model, we used greedy sampling, but the results were not as good as the previous model so we did not use it in our user tests. This model misses the main point of the article and replaces the subject with something different (ex: man getting mauled by a lion is turned into panda born at a zoo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ac4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the tokenizer\n",
    "with open('LSTM3model/x_tokenizer.pickle', 'rb') as handle:\n",
    "    x_tokenizer = pickle.load(handle)\n",
    "with open('LSTM3model/y_tokenizer.pickle', 'rb') as handle:\n",
    "    y_tokenizer = pickle.load(handle)\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4137f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary size from when we were training the encoder decoder model\n",
    "x_voc = 87194\n",
    "y_voc = 19878\n",
    "max_text_len=700\n",
    "max_summary_len=50\n",
    "\n",
    "# load the weights\n",
    "w_encoder_embeddings = np.load('LSTM3model/embedding.npz', allow_pickle=True)\n",
    "w_decoder_embeddings = np.load('LSTM3model/embedding_1.npz', allow_pickle=True)\n",
    "w_lstm = np.load('LSTM3model/lstm.npz', allow_pickle=True)\n",
    "w_lstm1 = np.load('LSTM3model/lstm_1.npz', allow_pickle=True)\n",
    "w_lstm2 = np.load('LSTM3model/lstm_2.npz', allow_pickle=True)\n",
    "w_lstm3 = np.load('LSTM3model/lstm_3.npz', allow_pickle=True)\n",
    "time_distributed  = np.load('LSTM3model/time_distributed.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(87194, 100, trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(19878, 100,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention(name='attention_layer')\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() \n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ece036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights of the model\n",
    "\n",
    "model.layers[1].set_weights(w_encoder_embeddings['arr_0'])\n",
    "model.layers[2].set_weights(w_lstm['arr_0'])\n",
    "model.layers[4].set_weights(w_lstm1['arr_0'])\n",
    "model.layers[5].set_weights(w_decoder_embeddings['arr_0'])\n",
    "model.layers[6].set_weights(w_lstm2['arr_0'])\n",
    "model.layers[7].set_weights(w_lstm3['arr_0'])\n",
    "model.layers[10].set_weights(time_distributed['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537f05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00283300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def top_k_sampling(predictions, k):\n",
    "    top_k_probabilities, top_k_indices= top_k(predictions, k=k, sorted=True)\n",
    "    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
    "    top_k_redistributed_probability=softmax(np.log(top_k_probabilities))\n",
    "    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
    "    token = np.random.choice(top_k_indices, p=top_k_redistributed_probability)\n",
    "    return token\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        # Uncomment one of the following lines to change the sampling method\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        #sampled_token_index = top_k_sampling(output_tokens[0, -1, :],5)\n",
    "        #sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=output_tokens[0, -1, :])\n",
    "  \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e917bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    x_val_seq   =   x_tokenizer.texts_to_sequences([text])\n",
    "    x_val_i   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "    return decode_sequence(x_val_i[0].reshape(1,max_text_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477cad11",
   "metadata": {},
   "source": [
    "#### Article\n",
    "Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.Â 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n",
    "#### Original Summary\n",
    "Experts question if  packed out planes are putting passengers at risk .\n",
    "U.S consumer advisory group says minimum space must be stipulated .\n",
    "Safety tests conducted on planes with more leg room than airlines offer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd7e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it\\'s putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn\\'t stipulate a minimum amount of space for humans. \\'In a world where animals have more rights to space and food than humans,\\' said Charlie Leocha, consumer representative on the committee.Â \\'It is time that the DOT and FAA take a stand for humane treatment of passengers.\\' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson\\'s short haul seat pitch is 28 inches, and Virgin Atlantic\\'s is 30-31.'\n",
    "cleaned_article = text_cleaner(article,0)\n",
    "print('Predicted summary:',summarize(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb5d10b",
   "metadata": {},
   "source": [
    "#### Article\n",
    "A drunk teenage boy had to be rescued by security after jumping into a lions' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at theÂ Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would 'kill them'. Mr Kumar explained afterwards that he was drunk and 'thought I'd stand a good chance' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions' enclosure at a zoo in Ahmedabad and began running towards the animals shouting 'Today I kill a lion!' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: 'Today I kill a lion or a lion kills me!' A zoo spokesman said: 'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. 'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. 'We then handed him over to the police.' Brave fool: Fortunately, Mr Kumar  fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: 'I don't really know why I did it. 'I was drunk and thought I'd stand a good chance.' A police spokesman said: 'He has been cautioned and will be sent for psychiatric evaluation. 'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.\n",
    "\n",
    "#### Original Summary\n",
    "Drunk teenage boy climbed into lion enclosure at zoo in west India .\n",
    "Rahul Kumar, 17, ran towards animals shouting 'Today I kill a lion!'\n",
    "Fortunately he fell into a moat before reaching lions and was rescued ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321db963",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = 'A drunk teenage boy had to be rescued by security after jumping into a lions\\' enclosure at a zoo in western India. Rahul Kumar, 17, clambered over the enclosure fence at theÂ Kamla Nehru Zoological Park in Ahmedabad, and began running towards the animals, shouting he would \\'kill them\\'. Mr Kumar explained afterwards that he was drunk and \\'thought I\\'d stand a good chance\\' against the predators. Next level drunk: Intoxicated Rahul Kumar, 17, climbed into the lions\\' enclosure at a zoo in Ahmedabad and began running towards the animals shouting \\'Today I kill a lion!\\' Mr Kumar had been sitting near the enclosure when he suddenly made a dash for the lions, surprising zoo security. The intoxicated teenager ran towards the lions, shouting: \\'Today I kill a lion or a lion kills me!\\' A zoo spokesman said: \\'Guards had earlier spotted him close to the enclosure but had no idea he was planing to enter it. \\'Fortunately, there are eight moats to cross before getting to where the lions usually are and he fell into the second one, allowing guards to catch up with him and take him out. \\'We then handed him over to the police.\\' Brave fool: Fortunately, Mr Kumar fell into a moat as he ran towards the lions and could be rescued by zoo security staff before reaching the animals (stock image) Kumar later explained: \\'I don\\'t really know why I did it. \\'I was drunk and thought I\\'d stand a good chance.\\' A police spokesman said: \\'He has been cautioned and will be sent for psychiatric evaluation. \\'Fortunately for him, the lions were asleep and the zoo guards acted quickly enough to prevent a tragedy similar to that in Delhi.\\' Last year a 20-year-old man was mauled to death by a tiger in the Indian capital after climbing into its enclosure at the city zoo.'\n",
    "cleaned_article = text_cleaner(article,0)\n",
    "print('Predicted summary:',summarize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec3d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d16ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:175]",
   "language": "python",
   "name": "conda-env-175-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
