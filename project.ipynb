{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9a3854",
   "metadata": {},
   "source": [
    "#### Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77483029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pathlib\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.math import top_k\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb46b1",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "We used the preprocessing code from https://iq.opengenus.org/text-summarization-using-rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b05e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString).text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if (num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1: # removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f976ac",
   "metadata": {},
   "source": [
    "### Load in our model\n",
    "\n",
    "The model we will be using is the encoder decoder model we wrote with 2 bidirectional LSTM layers in the encoder, and 1 LSTM layer in the decoder with an attention layer and dense layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e0d3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary size from when we were training the encoder decoder model\n",
    "x_voc = 8440\n",
    "y_voc = 1989\n",
    "\n",
    "# load the weights\n",
    "w_encoder_embeddings = np.load('model/enc_embedding.npz', allow_pickle=True)\n",
    "w_decoder_embeddings = np.load('model/dec_embedding.npz', allow_pickle=True)\n",
    "w_encoder_lstm0 = np.load('model/encoder_lstm0.npz', allow_pickle=True)\n",
    "w_encoder_lstm1 = np.load('model/encoder_lstm1.npz', allow_pickle=True)\n",
    "w_decoder_lstm = np.load('model/lstm_2.npz', allow_pickle=True)\n",
    "time_distributed  = np.load('model/time_distributed.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b91375da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_embedding (Embedding)       (None, None, 100)    6332000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm0 (Bidirectional)   (None, None, 600)    962400      enc_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm1 (Bidirectional)   [(None, 600), (None, 2162400     encoder_lstm0[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dec_embedding (Embedding)       (None, None, 100)    1485600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600)          0           encoder_lstm1[0][1]              \n",
      "                                                                 encoder_lstm1[0][3]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           encoder_lstm1[0][2]              \n",
      "                                                                 encoder_lstm1[0][4]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 600),  1682400     dec_embedding[0][0]              \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (Attention)     (None, None, 600)    0           lstm_2[0][0]                     \n",
      "                                                                 encoder_lstm1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 1200)   0           lstm_2[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 14856)  17842056    concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 30,466,856\n",
      "Trainable params: 30,466,856\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM encoder decoder\n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 300\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# embedding layer\n",
    "enc_emb =  Embedding(63320, 100, trainable=True, name = \"enc_embedding\")(encoder_inputs)\n",
    "\n",
    "\n",
    "encoder_lstm0 = Bidirectional(\n",
    "    LSTM(latent_dim, return_sequences=True),\n",
    "    name = 'encoder_lstm0')\n",
    "encoder_outputs0 = encoder_lstm0(enc_emb)\n",
    "\n",
    "encoder_lstm1 = Bidirectional(\n",
    "    LSTM(latent_dim, return_state=True),\n",
    "    name = 'encoder_lstm1')\n",
    "encoder_outputs, enc_forward_h, enc_forward_c, enc_backward_h, enc_backward_c  = encoder_lstm1(encoder_outputs0)\n",
    "\n",
    "\n",
    "state_h = Concatenate()([enc_forward_h, enc_backward_h])\n",
    "state_c = Concatenate()([enc_forward_c, enc_backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(14856, 100,trainable=True, name = 'dec_embedding')\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True,return_state=True,dropout=0.2,recurrent_dropout=0)\n",
    "decoder_outputs1, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=encoder_states)\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = Attention(name='attention_layer')\n",
    "attn_out = attn_layer([decoder_outputs1, encoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs1, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense = TimeDistributed(Dense(14856, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() \n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ede8d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weights of the model\n",
    "\n",
    "model.layers[1].set_weights(w_encoder_embeddings['arr_0'])\n",
    "model.layers[2].set_weights(w_encoder_lstm0['arr_0'])\n",
    "model.layers[4].set_weights(w_encoder_lstm1['arr_0'])\n",
    "model.layers[5].set_weights(w_decoder_embeddings['arr_0'])\n",
    "model.layers[8].set_weights(w_decoder_lstm['arr_0'])\n",
    "model.layers[11].set_weights(time_distributed['arr_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01378a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the tokenizer\n",
    "with open('model/x_tokenizer.pickle', 'rb') as handle:\n",
    "    x_tokenizer = pickle.load(handle)\n",
    "with open('model/y_tokenizer.pickle', 'rb') as handle:\n",
    "    y_tokenizer = pickle.load(handle)\n",
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f330531e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference model\n",
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
    "decoder_hidden_state_input = Input(shape=(latent_dim*2))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59534ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) \n",
    "\n",
    "def top_k_sampling(predictions, k):\n",
    "    top_k_probabilities, top_k_indices= top_k(predictions, k=k, sorted=True)\n",
    "    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
    "    top_k_redistributed_probability=softmax(np.log(top_k_probabilities))\n",
    "    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n",
    "    token = np.random.choice(top_k_indices, p=top_k_redistributed_probability)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1905f18",
   "metadata": {},
   "source": [
    "The following code is also code from  https://iq.opengenus.org/text-summarization-using-rnn/ but we changed the sampling methods and added our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b46a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        # Uncomment one of the following lines to change the sampling method\n",
    "        #sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token_index = top_k_sampling(output_tokens[0, -1, :],5)\n",
    "        #sampled_token_index = np.random.choice(len(output_tokens[0, -1, :]), p=output_tokens[0, -1, :])\n",
    "  \n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee5f6c",
   "metadata": {},
   "source": [
    "### Run some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70d4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len=400\n",
    "max_summary_len=46\n",
    "def summarize(text):\n",
    "    x_val_seq   =   x_tokenizer.texts_to_sequences([text])\n",
    "    x_val_i   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "    return decode_sequence(x_val_i[0].reshape(1,max_text_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de7dfd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the was captured on the river in east africa the tourist was captured by an animal and its spotted the tourists were captured by the tourists and they were taken to the scene the animal animal had been hit by the animal and rescue it\n"
     ]
    }
   ],
   "source": [
    "article = 'moment crew firefighters struggled haul giant pig garden swimming pool prize porker known fallen pool upmarket neighbourhood ringwood hampshire owners taking walk around garden animal plunged water unable get team dorset fire rescue struggled haul huge black pig swimming pool water prize porker known fallen water unable get two fire crews specialist animal rescue team use slide boards haul huge black pig small pool spokesman dorset fire rescue service said pm yesterday service received call pig stuck swimming pool one crew firefighters specialist animal rescue unit poole mobilised incident attendance crew secured pig requested attendance another appliance mobilised ringwood colleagues hampshire fire rescue service firefighters also called horse fallen swimming pool west sussex exhausted animal winched using terrain crane appeared worse wear tumble crew rescued pig swimming pool using specialist animal rescue slide boards lines haul pig swimming pool animal needed rescuing taking unexpected swim crews west sussex called swimming pool time horse fallen wet exhausted animal put opposition firefighters arrived hoist small garden pool two hour rescue operation ended wayward horse fitted straps belly lifted air terrain crane swung around deposited back dry land fire brigade spokesman said appeared none worse impromptu swim stepping edge domestic pool '\n",
    "print(summarize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba094118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the daily is expected to be in the first time in the last year the two countries have been in the form and is being offered by the company\n"
     ]
    }
   ],
   "source": [
    "article = 'Microsoft chairman bill gates late wednesday unveiled his vision of the digital lifestyle , outlining the latest version of his windows operating system to be launched later this year'\n",
    "\n",
    "cleaned_article = text_cleaner(article,0)\n",
    "print(summarize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96ff763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:175]",
   "language": "python",
   "name": "conda-env-175-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
